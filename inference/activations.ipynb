{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    MixtralModel,\n",
    "    MixtralConfig,\n",
    ")\n",
    "from torch import nn\n",
    "import collections\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1c797b7eb14af48e103a7464ce23e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer = 15\n",
    "\n",
    "double_quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "config = MixtralConfig(\n",
    "    num_experts_per_tok=8,\n",
    "    num_hidden_layers=layer,\n",
    ")\n",
    "\n",
    "model = MixtralModel(config).from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    quantization_config=double_quant_config,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generated_dataset(\n",
    "    filename, batch_size=32, dataset_relative_path=\"../dataset\"\n",
    "):\n",
    "    dataset = pd.read_csv(f\"{dataset_relative_path}/{filename}\")\n",
    "    # print(dataset.head(10))\n",
    "    dataset_list = dataset[\"0\"].tolist()\n",
    "    dataloader = torch.utils.data.DataLoader(dataset_list, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "dataset = load_generated_dataset(\"pile.csv\", batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = 0\n",
    "ACT_FN = 1\n",
    "\n",
    "sequence_length = 128\n",
    "batch_size = 6\n",
    "num_experts = 8\n",
    "expert_dims = 14336\n",
    "\n",
    "layer = 14\n",
    "\n",
    "experts = model.layers[layer].block_sparse_moe.experts\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1939 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     11\u001b[0m     hooks\u001b[38;5;241m.\u001b[39mextend([w3_hook, act_fn_hook])\n\u001b[1;32m     13\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m     14\u001b[0m batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m model(batch_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in tqdm(dataset):\n",
    "    if i > 0:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "    w3 = torch.zeros(num_experts, sequence_length * batch_size, expert_dims)\n",
    "    act_fn = torch.zeros(num_experts, sequence_length * batch_size, expert_dims)\n",
    "\n",
    "\n",
    "    def getActivation(expert_idx, type):\n",
    "        def hook(model, input, output):\n",
    "            if type == W3:\n",
    "                w3[expert_idx] = output.detach()\n",
    "            elif type == ACT_FN:\n",
    "                act_fn[expert_idx] = output.detach()\n",
    "\n",
    "        return hook\n",
    "\n",
    "    for expert_idx in range(num_experts):\n",
    "        experts[expert_idx].w3.register_forward_hook(getActivation(expert_idx, W3))\n",
    "        experts[expert_idx].act_fn.register_forward_hook(getActivation(expert_idx, ACT_FN))\n",
    "\n",
    "    hooks = []\n",
    "    for expert_idx in range(num_experts):\n",
    "        w3_hook = experts[expert_idx].w3.register_forward_hook(getActivation(expert_idx, W3))\n",
    "        act_fn_hook = experts[expert_idx].act_fn.register_forward_hook(getActivation(expert_idx, ACT_FN))\n",
    "        hooks.extend([w3_hook, act_fn_hook])\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    batch_tokens = tokenizer(\n",
    "        batch,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    output = model(batch_tokens[\"input_ids\"])\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # print(tokenizer.batch_decode(generated_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " layer_15_expert_0_act_w1 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_0_act_w1 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_0_w_3 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_0_w_3 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_1_act_w1 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_1_act_w1 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_1_w_3 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_1_w_3 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_2_act_w1 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_2_act_w1 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_2_w_3 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_2_w_3 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_3_act_w1 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_3_act_w1 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_3_w_3 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_3_w_3 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_4_act_w1 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_4_act_w1 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_4_w_3 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_4_w_3 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_5_act_w1 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_5_act_w1 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_5_w_3 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_5_w_3 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_6_act_w1 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_6_act_w1 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_6_w_3 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_6_w_3 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_7_act_w1 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_7_act_w1 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " layer_15_expert_7_w_3 , num_sequences 4\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])\n",
      " expert_7_w_3 , num_sequences 8\n",
      "torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])torch.Size([768, 14336])"
     ]
    }
   ],
   "source": [
    "for k, v in activations.items():\n",
    "    print(\"\\n\", k, \",\",'num_sequences', len(v))\n",
    "    for i in range(len(v)):\n",
    "        print(v[i].shape, end=\"\")\n",
    "        # print(v[i])\n",
    "        # break\n",
    "\n",
    "# for expert in range(8):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
