{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    MixtralModel,\n",
    "    MixtralConfig,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from torch import nn\n",
    "import collections\n",
    "import time\n",
    "import os\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 15\n",
    "\n",
    "W3 = 0\n",
    "ACT_FN = 1\n",
    "\n",
    "sequence_length = 128\n",
    "batch_size = 6\n",
    "num_experts = 8\n",
    "expert_dims = 14336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212c7883e9c9414785f3e9fa520f48f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "double_quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "config = MixtralConfig(\n",
    "    num_experts_per_tok=8,\n",
    "    num_hidden_layers=layer + 1,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    quantization_config=double_quant_config,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generated_dataset(\n",
    "    filename, batch_size, dataset_relative_path=\"../dataset\"\n",
    "):\n",
    "    dataset = pd.read_csv(f\"{dataset_relative_path}/{filename}\")\n",
    "    # print(dataset.head(10))\n",
    "    dataset_list = dataset[\"0\"].tolist()\n",
    "    dataloader = torch.utils.data.DataLoader(dataset_list, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "dataset = load_generated_dataset(\"pile.csv\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1939 [00:03<1:38:00,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.4424e+00,  1.0535e-01, -1.5508e+00,  ...,  5.1465e-01,\n",
      "           6.0400e-01, -4.7119e-02],\n",
      "         [-9.0820e-01, -3.0933e-01, -1.3018e+00,  ...,  3.8281e-01,\n",
      "           4.7241e-01, -3.0566e-01],\n",
      "         [-8.4229e-01, -1.4343e-01, -9.5020e-01,  ...,  1.1152e+00,\n",
      "           3.5205e-01, -7.7393e-01],\n",
      "         ...,\n",
      "         [-1.8525e+00, -3.8208e-01, -4.5483e-01,  ..., -7.9883e-01,\n",
      "          -1.5820e-01,  1.8091e-01],\n",
      "         [ 1.1494e+00,  3.1689e-01,  2.3938e-01,  ...,  9.6375e-02,\n",
      "          -4.3945e-01, -2.6904e-01],\n",
      "         [ 1.2168e+00,  1.9617e-01, -1.2732e-01,  ...,  1.8958e-01,\n",
      "          -3.1348e-01, -6.5723e-01]],\n",
      "\n",
      "        [[ 1.7432e-01,  8.7256e-01, -6.5576e-01,  ...,  4.9194e-01,\n",
      "          -2.8076e-01, -4.7394e-02],\n",
      "         [-2.4902e-01, -5.9863e-01, -8.7256e-01,  ..., -3.7207e-01,\n",
      "           5.1318e-01,  6.6992e-01],\n",
      "         [ 1.9092e-01, -1.4294e-01, -1.4561e+00,  ...,  3.0908e-01,\n",
      "           9.8779e-01, -1.9507e-01],\n",
      "         ...,\n",
      "         [ 4.3945e-01,  3.6646e-01, -1.1299e+00,  ..., -3.4570e-01,\n",
      "          -4.1290e-02, -1.3467e+00],\n",
      "         [ 3.9215e-02, -2.0728e-01, -9.2334e-01,  ..., -2.4023e-01,\n",
      "          -4.1333e-01, -6.3330e-01],\n",
      "         [ 4.6143e-01, -8.8232e-01, -7.2217e-01,  ...,  1.9971e-01,\n",
      "           8.4326e-01, -4.9048e-01]],\n",
      "\n",
      "        [[-1.0449e+00, -9.5032e-02, -7.7881e-01,  ..., -3.4912e-02,\n",
      "          -1.0236e-01, -3.8086e-01],\n",
      "         [-4.8608e-01, -5.1758e-01, -1.2324e+00,  ...,  6.6748e-01,\n",
      "           8.4229e-01, -4.7791e-02],\n",
      "         [ 1.4819e-01, -1.6199e-01, -1.1758e+00,  ...,  3.0005e-01,\n",
      "           3.5181e-01, -1.9849e-01],\n",
      "         ...,\n",
      "         [ 4.6338e-01, -1.0898e+00, -1.1318e+00,  ..., -6.7759e-04,\n",
      "           1.7102e-01, -3.2532e-02],\n",
      "         [ 1.2891e+00,  1.7188e-01, -6.5283e-01,  ...,  1.3828e+00,\n",
      "          -1.8539e-02, -2.8296e-01],\n",
      "         [ 1.6826e+00, -1.0187e-01,  2.0447e-01,  ..., -1.2805e-01,\n",
      "          -5.0342e-01, -1.2061e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.6543e+00,  3.2861e-01, -5.5908e-01,  ..., -9.6191e-02,\n",
      "           7.2314e-01,  2.7539e-01],\n",
      "         [ 1.5791e+00, -1.2598e-01, -1.0996e+00,  ...,  5.8252e-01,\n",
      "           8.1152e-01, -2.6294e-01],\n",
      "         [ 1.0010e+00,  6.5088e-01,  3.1909e-01,  ...,  2.8076e-01,\n",
      "          -3.1274e-01, -4.7632e-01],\n",
      "         ...,\n",
      "         [ 1.5259e-01,  5.1465e-01, -2.0581e-01,  ...,  4.0436e-02,\n",
      "           5.1123e-01, -5.4840e-02],\n",
      "         [ 1.8184e+00,  8.8623e-01,  1.7861e+00,  ...,  1.0508e+00,\n",
      "           5.5566e-01, -1.8457e-01],\n",
      "         [ 3.8721e-01,  5.9766e-01, -3.0981e-01,  ...,  3.9941e-01,\n",
      "           6.4893e-01,  2.4463e-01]],\n",
      "\n",
      "        [[ 5.8643e-01, -9.9304e-02, -2.9453e+00,  ...,  9.3066e-01,\n",
      "           4.7729e-02, -9.7510e-01],\n",
      "         [-1.8286e-01, -2.0068e-01,  2.0239e-01,  ...,  1.7061e+00,\n",
      "           1.9080e-01, -2.1484e+00],\n",
      "         [-9.3311e-01, -3.8916e-01, -4.3970e-01,  ...,  6.7200e-02,\n",
      "           7.0068e-01, -8.8135e-01],\n",
      "         ...,\n",
      "         [-1.7273e-01,  1.4001e-01, -1.1826e+00,  ..., -2.8882e-01,\n",
      "           3.1006e-01, -9.8486e-01],\n",
      "         [ 1.4612e-01,  4.3115e-01,  8.6853e-02,  ...,  1.1560e-01,\n",
      "           3.1006e-01, -8.4961e-01],\n",
      "         [-2.1935e-04, -3.4961e-01,  5.7648e-02,  ..., -6.7432e-01,\n",
      "          -5.6396e-01, -6.9580e-01]],\n",
      "\n",
      "        [[ 1.2671e-01,  3.6157e-01, -1.2549e+00,  ...,  2.2656e-01,\n",
      "           2.0264e-01,  4.2041e-01],\n",
      "         [-8.9905e-02,  1.0088e+00, -1.2773e+00,  ..., -8.7585e-02,\n",
      "           8.3350e-01, -1.8079e-01],\n",
      "         [-8.1836e-01,  1.6885e+00, -1.3955e+00,  ..., -2.1496e-03,\n",
      "           1.9229e+00, -6.1279e-01],\n",
      "         ...,\n",
      "         [ 3.1128e-01,  6.8321e-03, -7.3682e-01,  ..., -1.0699e-01,\n",
      "          -4.8492e-02,  2.8687e-01],\n",
      "         [-1.8237e-01, -1.7426e-02, -9.3896e-01,  ...,  1.9458e-01,\n",
      "           1.3904e-01,  2.2534e-01],\n",
      "         [-3.1494e-01, -7.4805e-01, -3.1396e-01,  ...,  8.4000e-03,\n",
      "           2.3376e-01,  3.3173e-02]]])\n",
      "torch.Size([768, 114688])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "experts = model.model.layers[layer].block_sparse_moe.experts\n",
    "\n",
    "\n",
    "for batch in tqdm(dataset):\n",
    "    if i > 0:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "    w3 = torch.zeros(num_experts, sequence_length * batch_size, expert_dims)\n",
    "    act_fn = torch.zeros(num_experts, sequence_length * batch_size, expert_dims)\n",
    "\n",
    "\n",
    "    def getActivation(expert_idx, type):\n",
    "        def hook(model, input, output):\n",
    "            if type == W3:\n",
    "                w3[expert_idx] = output.detach()\n",
    "            elif type == ACT_FN:\n",
    "                act_fn[expert_idx] = output.detach()\n",
    "\n",
    "        return hook\n",
    "\n",
    "    for expert_idx in range(num_experts):\n",
    "        experts[expert_idx].w3.register_forward_hook(getActivation(expert_idx, W3))\n",
    "        experts[expert_idx].act_fn.register_forward_hook(getActivation(expert_idx, ACT_FN))\n",
    "\n",
    "    hooks = []\n",
    "    for expert_idx in range(num_experts):\n",
    "        w3_hook = experts[expert_idx].w3.register_forward_hook(getActivation(expert_idx, W3))\n",
    "        act_fn_hook = experts[expert_idx].act_fn.register_forward_hook(getActivation(expert_idx, ACT_FN))\n",
    "        hooks.extend([w3_hook, act_fn_hook])\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    batch_tokens = tokenizer(\n",
    "        batch,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    output = model(batch_tokens[\"input_ids\"])\n",
    "\n",
    "    acts = rearrange(w3 * act_fn, 'experts sequences dims -> sequences (experts dims)')\n",
    "    print(acts.shape)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # print(tokenizer.batch_decode(generated_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
