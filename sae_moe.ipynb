{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134f8d7becef48fdbe00ceb09d6f2b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "double_quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    quantization_config=double_quant_config,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = collections.defaultdict(list)\n",
    "\n",
    "def getActivation(name):\n",
    "    def hook(model, input, output):\n",
    "        assert type(output) is not tuple\n",
    "        activation[name].append(output.detach())\n",
    "\n",
    "    return hook\n",
    "\n",
    "experts = model.model.layers[15].block_sparse_moe.experts\n",
    "\n",
    "for expert_idx in range(8):\n",
    "    experts[expert_idx].w3.register_forward_hook(getActivation(f\"layer_15_expert_{expert_idx}_w_3\"))\n",
    "    experts[expert_idx].act_fn.register_forward_hook(\n",
    "        getActivation(f\"layer_15_expert_{expert_idx}_act_w1\")\n",
    "    )\n",
    "    experts[expert_idx].w2.register_forward_hook(getActivation(f\"layer_15_expert_{expert_idx}_w_2\"))\n",
    "\n",
    "\n",
    "# gate_hook = model.model.layers[15].block_sparse_moe.gate.register_forward_hook(getActivation(\"layer_15_gate\"))\n",
    "\n",
    "# moe_hook = model.model.layers[15].block_sparse_moe.register_forward_hook(\n",
    "#     getActivation(\"layer_15_moe_block\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiLU()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[15].block_sparse_moe.experts[0].act_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> #\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=1, do_sample=False)\n",
    "print(tokenizer.batch_decode(generated_ids)[0])\n",
    "\n",
    "# print(activation['15_6'].shape)\n",
    "# print(activation[\"15_6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'layer_15_expert_4_act_w1': [tensor([[0.0084, 0.1205, 0.0468,  ..., 0.1027, 0.0519, 0.0596]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([[0.0084, 0.1205, 0.0468,  ..., 0.1027, 0.0519, 0.0596]],\n",
      "       device='cuda:0', dtype=torch.float16)], 'layer_15_expert_4_w_3': [tensor([[-0.0749, -0.0735, -0.3325,  ..., -0.3240,  0.2378, -0.1473]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([[-0.0749, -0.0735, -0.3325,  ..., -0.3240,  0.2378, -0.1473]],\n",
      "       device='cuda:0', dtype=torch.float16)], 'layer_15_expert_4_w_2': [tensor([[-0.0640,  0.1124, -0.1562,  ..., -0.0375,  0.0068, -0.0149]],\n",
      "       device='cuda:0', dtype=torch.float16)], 'layer_15_expert_6_act_w1': [tensor([[0.1271, 0.2178, 0.0239,  ..., 0.0504, 0.0281, 0.0688]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([[0.1271, 0.2178, 0.0239,  ..., 0.0504, 0.0281, 0.0688]],\n",
      "       device='cuda:0', dtype=torch.float16)], 'layer_15_expert_6_w_3': [tensor([[-0.1215,  0.2671, -0.3540,  ..., -0.2688,  0.1148,  0.0077]],\n",
      "       device='cuda:0', dtype=torch.float16), tensor([[-0.1215,  0.2671, -0.3540,  ..., -0.2688,  0.1148,  0.0077]],\n",
      "       device='cuda:0', dtype=torch.float16)], 'layer_15_expert_6_w_2': [tensor([[-0.3660,  0.0847, -0.2098,  ..., -0.2991,  0.1411,  0.0594]],\n",
      "       device='cuda:0', dtype=torch.float16)]})\n",
      "layer_15_expert_4_act_w1: torch.Size([1, 14336]) 2\n",
      "layer_15_expert_4_w_3: torch.Size([1, 14336]) 2\n",
      "layer_15_expert_4_w_2: torch.Size([1, 4096]) 1\n",
      "layer_15_expert_6_act_w1: torch.Size([1, 14336]) 2\n",
      "layer_15_expert_6_w_3: torch.Size([1, 14336]) 2\n",
      "layer_15_expert_6_w_2: torch.Size([1, 4096]) 1\n"
     ]
    }
   ],
   "source": [
    "print(activation)\n",
    "for key, value in activation.items():\n",
    "    print(f\"{key}: {value[0].shape} {len(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n",
      "torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "w2_acts = model.model.layers[15].block_sparse_moe.experts[4].w2(activation[\"layer_15_expert_4_act_w1\"][0] * activation[\"layer_15_expert_4_w_3\"][0])\n",
    "print(w2_acts.shape)\n",
    "print(activation[\"layer_15_expert_4_w_2\"][0].shape)\n",
    "assert(torch.allclose(w2_acts, activation[\"layer_15_expert_4_w_2\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29360128, 1])\n",
      "torch.Size([29360128, 1])\n",
      "torch.Size([29360128, 1])\n",
      "torch.Size([16384, 1])\n",
      "dict_keys(['count', 'layer_15_gate', 'layer_15_expert_4', 'layer_15_expert_6', 'layer_15_moe_block'])\n",
      "8\n",
      "torch.Size([1, 14336])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 1, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers[15].block_sparse_moe.experts[5].w1.weight.shape)\n",
    "print(model.model.layers[15].block_sparse_moe.experts[5].w2.weight.shape)\n",
    "print(model.model.layers[15].block_sparse_moe.experts[5].w3.weight.shape)\n",
    "\n",
    "print(model.model.layers[15].block_sparse_moe.gate.weight.shape)\n",
    "print(activation.keys())\n",
    "print(activation['count'])\n",
    "print(activation[\"layer_15_expert_4\"].shape)\n",
    "print(activation[\"layer_15_gate\"].shape)\n",
    "print(activation[\"layer_15_moe_block\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18766/2862377057.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m expert_mask = torch.nn.functional.one_hot(\n\u001b[1;32m      2\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m ).permute(2, 1, 0)\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "expert_mask = torch.nn.functional.one_hot(\n",
    "    torch.tensor([3, 2, 4]), num_classes=8\n",
    ").permute(2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
